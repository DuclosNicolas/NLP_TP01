{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TUIA NLP 2025 - TP Final\n"
      ],
      "metadata": {
        "id": "YjW6eH2dGzgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descarga de Datos"
      ],
      "metadata": {
        "id": "A-hgRyFkHEIP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JUds1X_1EkW",
        "outputId": "3ec31975-f6b0-4f82-c68b-5807250a2555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12HdLgCWfh2JlLVf_LnAVWWA3hrqut9mi\n",
            "To: /content/Datos.zip\n",
            "100%|██████████| 182k/182k [00:00<00:00, 46.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "file_id = '12HdLgCWfh2JlLVf_LnAVWWA3hrqut9mi'\n",
        "output = 'Datos.zip'\n",
        "\n",
        "# Descargo la carpeta de datos comprimida como .zip\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', output, quiet=False)\n",
        "\n",
        "# Descomprimir\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')  # Carpeta destino"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token: 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"
      ],
      "metadata": {
        "id": "G2ZKHSBRbO5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-community\n",
        "!pip install -q py2neo\n",
        "!pip install -U duckduckgo-search\n",
        "!pip install wikipedia\n",
        "!pip install -qU langchain-community faiss-cpu\n",
        "!pip install -q transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMTItzUIix4n",
        "outputId": "83d24cdd-6e3a-46b0-f684-3fed0494c440"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.0.4-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Downloading duckduckgo_search-8.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, duckduckgo-search\n",
            "Successfully installed duckduckgo-search-8.0.4 primp-0.15.0\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.14.0)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=eec80351f4496838f0677023697968e9ec89163a47b8aad65ac20d7e42d9fd61\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliotecas"
      ],
      "metadata": {
        "id": "qGQL4k6R0Eum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import sqlite3\n",
        "from typing import List, Dict, Any, Optional\n",
        "import warnings\n",
        "import requests\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Bibliotecas para embeddings y vectores\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Bibliotecas para LLM\n",
        "import openai\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Bibliotecas para grafos\n",
        "import networkx as nx\n",
        "from py2neo import Graph, Node, Relationship\n",
        "\n",
        "# Bibliotecas para agente\n",
        "from langchain.agents import Tool, AgentExecutor, create_react_agent\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
        "from langchain.utilities import WikipediaAPIWrapper\n"
      ],
      "metadata": {
        "id": "GW6ayaLS0KCV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1: Sistema RAG"
      ],
      "metadata": {
        "id": "TQ_25Naw0W-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyTownsRAG:\n",
        "  \"\"\"Sistema RAG para Tiny Towns\"\"\"\n",
        "\n",
        "  def __init__(self, data_path: str = \"/content/Datos\"):\n",
        "      self.data_path = data_path\n",
        "      self.embedding_model = None\n",
        "      self.vector_db = None\n",
        "      self.text_chunks = []\n",
        "      self.df_estadisticas = None\n",
        "      self.graph_db = None\n",
        "      self.intent_classifier = None\n",
        "\n",
        "      # Inicializar componentes\n",
        "      self._load_data()\n",
        "      self._setup_embedding_model()\n",
        "      self._create_vector_database()\n",
        "      self._setup_statistics_interface()\n",
        "      self._setup_graph_database()\n",
        "      self._setup_intent_classifier()\n",
        "\n",
        "  def _load_data(self):\n",
        "      \"\"\"Cargar todos los datos del TP1\"\"\"\n",
        "      print(\"Cargando datos...\")\n",
        "\n",
        "      # Cargar datos de información (textos)\n",
        "      self.info_texts = {}\n",
        "      info_path = os.path.join(self.data_path, \"Información\")\n",
        "      for file in os.listdir(info_path):\n",
        "          if file.endswith('.txt'):\n",
        "              with open(os.path.join(info_path, file), 'r', encoding='utf-8') as f:\n",
        "                  self.info_texts[file] = f.read()\n",
        "          elif file.endswith('.csv'):\n",
        "              df = pd.read_csv(os.path.join(info_path, file))\n",
        "              self.info_texts[file] = df.to_string()\n",
        "\n",
        "      # Cargar estadísticas\n",
        "      stats_path = os.path.join(self.data_path, \"Estadísticas\")\n",
        "      self.df_reseñas = pd.read_csv(os.path.join(stats_path, \"reseñas_Tiny_Towns.csv\"))\n",
        "      self.df_boardgame = pd.read_csv(os.path.join(stats_path, \"boardgame_data.csv\"))\n",
        "      self.df_credits = pd.read_csv(os.path.join(stats_path, \"credits.csv\"))\n",
        "\n",
        "      # Cargar relaciones\n",
        "      relations_path = os.path.join(self.data_path, \"Relaciones\")\n",
        "      self.df_relaciones = pd.read_csv(os.path.join(relations_path, \"creditos_relaciones.csv\"))\n",
        "\n",
        "      print(\"Datos cargados exitosamente\")\n",
        "\n",
        "  def _setup_embedding_model(self):\n",
        "      \"\"\"Configurar modelo de embeddings\"\"\"\n",
        "      print(\"Configurando modelo de embeddings...\")\n",
        "      # Usando sentence-transformers con modelo multilingüe\n",
        "      self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "      print(\"Modelo de embeddings configurado\")\n",
        "\n",
        "  def _create_text_chunks(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "      \"\"\"Fragmentar texto con overlapping\"\"\"\n",
        "      words = text.split()\n",
        "      chunks = []\n",
        "\n",
        "      for i in range(0, len(words), chunk_size - overlap):\n",
        "          chunk = ' '.join(words[i:i + chunk_size])\n",
        "          chunks.append(chunk)\n",
        "\n",
        "      return chunks\n",
        "\n",
        "  def _create_vector_database(self):\n",
        "      \"\"\"Crear base de datos vectorial con FAISS\"\"\"\n",
        "      print(\"Creando base de datos vectorial...\")\n",
        "\n",
        "      # Fragmentar todos los textos\n",
        "      all_chunks = []\n",
        "      chunk_metadata = []\n",
        "\n",
        "      for filename, text in self.info_texts.items():\n",
        "          chunks = self._create_text_chunks(text)\n",
        "          all_chunks.extend(chunks)\n",
        "          chunk_metadata.extend([{\"source\": filename, \"chunk_id\": i} for i in range(len(chunks))])\n",
        "\n",
        "      self.text_chunks = all_chunks\n",
        "      self.chunk_metadata = chunk_metadata\n",
        "\n",
        "      # Crear embeddings\n",
        "      embeddings = self.embedding_model.encode(all_chunks)\n",
        "\n",
        "      # Crear índice FAISS\n",
        "      dimension = embeddings.shape[1]\n",
        "      self.vector_db = faiss.IndexFlatIP(dimension)  # Inner Product para cosine similarity\n",
        "\n",
        "      # Normalizar embeddings para cosine similarity\n",
        "      faiss.normalize_L2(embeddings)\n",
        "      self.vector_db.add(embeddings.astype('float32'))\n",
        "\n",
        "      # Configurar TF-IDF para búsqueda híbrida\n",
        "      self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "      self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_chunks)\n",
        "\n",
        "      print(f\"Base de datos vectorial creada con {len(all_chunks)} chunks\")\n",
        "\n",
        "  def semantic_search(self, query: str, k: int = 5) -> List[Dict]:\n",
        "      \"\"\"Búsqueda semántica en la base de datos vectorial\"\"\"\n",
        "      # Embedding de la consulta\n",
        "      query_embedding = self.embedding_model.encode([query])\n",
        "      faiss.normalize_L2(query_embedding)\n",
        "\n",
        "      # Búsqueda en FAISS\n",
        "      scores, indices = self.vector_db.search(query_embedding.astype('float32'), k)\n",
        "\n",
        "      results = []\n",
        "      for i, idx in enumerate(indices[0]):\n",
        "          results.append({\n",
        "              \"text\": self.text_chunks[idx],\n",
        "              \"score\": float(scores[0][i]),\n",
        "              \"metadata\": self.chunk_metadata[idx]\n",
        "          })\n",
        "\n",
        "      return results\n",
        "\n",
        "  def hybrid_search(self, query: str, k: int = 5, alpha: float = 0.7) -> List[Dict]:\n",
        "      \"\"\"Búsqueda híbrida: semántica + BM25/TF-IDF\"\"\"\n",
        "      # Búsqueda semántica\n",
        "      semantic_results = self.semantic_search(query, k * 2)\n",
        "\n",
        "      # Búsqueda por palabras clave (TF-IDF)\n",
        "      query_tfidf = self.tfidf_vectorizer.transform([query])\n",
        "      tfidf_scores = cosine_similarity(query_tfidf, self.tfidf_matrix).flatten()\n",
        "\n",
        "      # Combinar scores\n",
        "      final_scores = {}\n",
        "\n",
        "      # Agregar scores semánticos\n",
        "      for result in semantic_results:\n",
        "          idx = self.text_chunks.index(result[\"text\"])\n",
        "          final_scores[idx] = alpha * result[\"score\"]\n",
        "\n",
        "      # Agregar scores TF-IDF\n",
        "      for idx, score in enumerate(tfidf_scores):\n",
        "          if idx in final_scores:\n",
        "              final_scores[idx] += (1 - alpha) * score\n",
        "          else:\n",
        "              final_scores[idx] = (1 - alpha) * score\n",
        "\n",
        "      # Ordenar y retornar top k\n",
        "      sorted_indices = sorted(final_scores.keys(), key=lambda x: final_scores[x], reverse=True)[:k]\n",
        "\n",
        "      results = []\n",
        "      for idx in sorted_indices:\n",
        "          results.append({\n",
        "              \"text\": self.text_chunks[idx],\n",
        "              \"score\": final_scores[idx],\n",
        "              \"metadata\": self.chunk_metadata[idx]\n",
        "          })\n",
        "\n",
        "      return results\n",
        "\n",
        "  def rerank_results(self, query: str, results: List[Dict]) -> List[Dict]:\n",
        "      \"\"\"Re-ranking de resultados usando cross-encoder\"\"\"\n",
        "      # Implementación usando similarity\n",
        "      texts = [result[\"text\"] for result in results]\n",
        "      query_embedding = self.embedding_model.encode([query])\n",
        "      text_embeddings = self.embedding_model.encode(texts)\n",
        "\n",
        "      similarities = cosine_similarity(query_embedding, text_embeddings).flatten()\n",
        "\n",
        "      # Actualizar scores con re-ranking\n",
        "      for i, result in enumerate(results):\n",
        "          result[\"rerank_score\"] = similarities[i]\n",
        "\n",
        "      # Ordenar por nuevo score\n",
        "      results.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "      return results\n",
        "\n",
        "  def _setup_statistics_interface(self):\n",
        "      \"\"\"Configurar interfaz para datos estadísticos\"\"\"\n",
        "      print(\"Configurando interfaz estadística...\")\n",
        "\n",
        "      # Analizar estadísticas de los datasets\n",
        "      self.stats_info = {\n",
        "          \"reseñas\": {\n",
        "              \"columns\": list(self.df_reseñas.columns),\n",
        "              \"categorical\": [],\n",
        "              \"numerical\": [],\n",
        "              \"unique_values\": {}\n",
        "          },\n",
        "          \"boardgame\": {\n",
        "              \"columns\": list(self.df_boardgame.columns),\n",
        "              \"categorical\": [],\n",
        "              \"numerical\": [],\n",
        "              \"unique_values\": {}\n",
        "          }\n",
        "      }\n",
        "\n",
        "      # Analizar tipos de datos\n",
        "      for dataset_name, df in [(\"reseñas\", self.df_reseñas), (\"boardgame\", self.df_boardgame)]:\n",
        "          for col in df.columns:\n",
        "              if df[col].dtype in ['object', 'string']:\n",
        "                  self.stats_info[dataset_name][\"categorical\"].append(col)\n",
        "                  unique_vals = df[col].unique()[:10]  # Primeros 10 valores únicos\n",
        "                  self.stats_info[dataset_name][\"unique_values\"][col] = list(unique_vals)\n",
        "              else:\n",
        "                  self.stats_info[dataset_name][\"numerical\"].append(col)\n",
        "                  self.stats_info[dataset_name][\"unique_values\"][col] = {\n",
        "                      \"min\": float(df[col].min()),\n",
        "                      \"max\": float(df[col].max()),\n",
        "                      \"mean\": float(df[col].mean())\n",
        "                  }\n",
        "\n",
        "      print(\"Interfaz estadística configurada\")\n",
        "\n",
        "  def generate_sql_filter(self, query: str, dataset: str = \"reseñas\") -> str:\n",
        "      \"\"\"Generar filtro Pandas usando LLM de Hugging Face\"\"\"\n",
        "      import requests\n",
        "\n",
        "      api_key = \"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\"\n",
        "      api_url = \"https://api-inference.huggingface.co/models/google/gemma-2b\"\n",
        "      headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "      dataset_info = self.stats_info.get(dataset, {})\n",
        "      # Prompt\n",
        "      prompt = (\n",
        "          f\"Columnas: {dataset_info.get('columns', [])}\\n\"\n",
        "          f\"Consulta del usuario: \\\"{query}\\\"\\n\"\n",
        "      )\n",
        "\n",
        "      # Estructura de chat para el modelo\n",
        "      chat_prompt = [\n",
        "          {\"role\": \"system\", \"content\": \"\"},\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "      # Unimos los mensajes en un solo string para el modelo Zephyr\n",
        "      prompt_with_template = \"\"\n",
        "      for msg in chat_prompt:\n",
        "          role = msg[\"role\"]\n",
        "          prompt_with_template += f\"<|{role}|>{msg['content']}</s>\\n\"\n",
        "      prompt_with_template += \"<|assistant|>\\n\"\n",
        "\n",
        "      data = {\n",
        "          \"inputs\": prompt_with_template,\n",
        "          \"parameters\": {\n",
        "              \"max_new_tokens\": 64,\n",
        "              \"temperature\": 0.1,\n",
        "              \"top_k\": 20,\n",
        "              \"top_p\": 0.95\n",
        "          }\n",
        "      }\n",
        "\n",
        "      try:\n",
        "          response = requests.post(api_url, headers=headers, json=data, timeout=30)\n",
        "          response.raise_for_status()\n",
        "          result = response.json()\n",
        "          generated = result[0]['generated_text'].strip().split('\\n')[0]\n",
        "          return generated\n",
        "      except Exception as e:\n",
        "          print(f\"Error llamando al LLM: {e}\")\n",
        "          return \"df\"  # Fallback seguro\n",
        "\n",
        "  def query_statistics(self, query: str, dataset: str = \"reseñas\") -> Dict:\n",
        "      \"\"\"Consultar datos estadísticos\"\"\"\n",
        "      # Generar filtro con LLM\n",
        "      filter_code = self.generate_sql_filter(query, dataset)\n",
        "\n",
        "      # Aplicar filtro\n",
        "      try:\n",
        "          df = self.df_reseñas if dataset == \"reseñas\" else self.df_boardgame\n",
        "          filtered_df = eval(filter_code)\n",
        "\n",
        "          return {\n",
        "              \"success\": True,\n",
        "              \"data\": filtered_df.head(10).to_dict('records'),\n",
        "              \"count\": len(filtered_df),\n",
        "              \"filter_used\": filter_code\n",
        "          }\n",
        "      except Exception as e:\n",
        "          return {\n",
        "              \"success\": False,\n",
        "              \"error\": str(e),\n",
        "              \"filter_used\": filter_code\n",
        "          }\n",
        "\n",
        "  def _setup_graph_database(self):\n",
        "    \"\"\"Configurar base de datos de grafos con Neo4j\"\"\"\n",
        "    print(\"Configurando base de datos de grafos...\")\n",
        "\n",
        "    try:\n",
        "        from py2neo import Graph\n",
        "        self.graph_db = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Cambia el password si es necesario\n",
        "\n",
        "        # Limpiar datos existentes\n",
        "        self.graph_db.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "        # Importar datos desde el dataframe\n",
        "        for _, row in self.df_relaciones.iterrows():\n",
        "            source = str(row['Sujeto'])\n",
        "            target = str(row['Objeto'])\n",
        "            rel_type = str(row['Relación']).replace(\" \", \"_\").upper()  # Cypher no permite espacios en el tipo de relación\n",
        "\n",
        "            # Crear nodos y relación\n",
        "            self.graph_db.run(\n",
        "                f\"MERGE (a:Entity {{name: $source}}) \"\n",
        "                f\"MERGE (b:Entity {{name: $target}}) \"\n",
        "                f\"MERGE (a)-[r:{rel_type}]->(b)\",\n",
        "                source=source, target=target\n",
        "            )\n",
        "\n",
        "        print(f\"Grafo Neo4j creado con {len(self.df_relaciones)} relaciones\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configurando Neo4j: {str(e)}\")\n",
        "\n",
        "  def generate_cypher_query(self, query: str) -> str:\n",
        "    \"\"\"Generar consulta Cypher usando LLM de Hugging Face\"\"\"\n",
        "    import requests\n",
        "\n",
        "    api_key = \"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\"\n",
        "    api_url = \"https://api-inference.huggingface.co/models/google/gemma-2b\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "    # Extrae relaciones únicas para ayudar al LLM\n",
        "    relaciones = list(self.df_relaciones['Relación'].unique())\n",
        "    prompt = (\n",
        "        \"Eres un experto en grafos y Cypher para Neo4j.\\n\"\n",
        "        \"El grafo tiene nodos con el atributo 'name' y relaciones de tipo:\\n\"\n",
        "        f\"{', '.join(relaciones)}\\n\"\n",
        "        f\"Convierte la siguiente consulta en lenguaje natural a una consulta Cypher.\\n\"\n",
        "        f\"Consulta: \\\"{query}\\\"\\n\"\n",
        "        \"Devuelve solo la consulta Cypher, sin explicaciones ni comentarios.\"\n",
        "    )\n",
        "\n",
        "    chat_prompt = [\n",
        "        {\"role\": \"system\", \"content\": \"Eres un experto en Cypher y grafos. Devuelve solo la consulta Cypher para la consulta dada.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    prompt_with_template = \"\"\n",
        "    for msg in chat_prompt:\n",
        "        role = msg[\"role\"]\n",
        "        prompt_with_template += f\"<|{role}|>{msg['content']}</s>\\n\"\n",
        "    prompt_with_template += \"<|assistant|>\\n\"\n",
        "\n",
        "    data = {\n",
        "        \"inputs\": prompt_with_template,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 64,\n",
        "            \"temperature\": 0.1,\n",
        "            \"top_k\": 20,\n",
        "            \"top_p\": 0.95\n",
        "        }\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_url, headers=headers, json=data, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        generated = result[0]['generated_text'].strip().split('\\n')[0]\n",
        "        return generated\n",
        "    except Exception as e:\n",
        "        print(f\"Error llamando al LLM: {e}\")\n",
        "        return \"MATCH (n) RETURN n LIMIT 10\"\n",
        "\n",
        "  def query_graph(self, query: str) -> Dict:\n",
        "    \"\"\"Consultar base de datos de grafos con Cypher generado por LLM\"\"\"\n",
        "    try:\n",
        "        # Generar consulta Cypher\n",
        "        cypher_query = self.generate_cypher_query(query)\n",
        "\n",
        "        if hasattr(self, 'graph_db'):\n",
        "            # Consultar Neo4j\n",
        "            result = self.graph_db.run(cypher_query).data()\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"data\": result,\n",
        "                \"query_used\": cypher_query\n",
        "            }\n",
        "        else:\n",
        "            # Simular consulta Cypher en NetworkX (para desarrollo)\n",
        "            # Esto es un aproximado muy básico\n",
        "            nodes = list(self.graph.nodes())\n",
        "            edges = list(self.graph.edges(data=True))\n",
        "\n",
        "            # Filtrar basado en términos de la consulta\n",
        "            filtered_nodes = [n for n in nodes if any(q.lower() in str(n).lower() for q in query.split())]\n",
        "            filtered_edges = [e for e in edges if any(q.lower() in str(e).lower() for q in query.split())]\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"nodes\": filtered_nodes[:10],\n",
        "                \"edges\": filtered_edges[:10],\n",
        "                \"query_used\": cypher_query\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e),\n",
        "            \"query_used\": cypher_query if 'cypher_query' in locals() else \"N/A\"\n",
        "        }\n",
        "\n",
        "  def _setup_intent_classifier(self):\n",
        "      \"\"\"Configurar clasificador de intención mejorado\"\"\"\n",
        "      print(\"Configurando clasificador de intención avanzado...\")\n",
        "\n",
        "      # Opción 1: Modelo entrenado\n",
        "      try:\n",
        "          self.intent_model = pipeline(\n",
        "              \"sentiment-analysis\",\n",
        "              model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",  # Ruta al modelo entrenado en TP1\n",
        "              tokenizer=\"path/to/tokenizer\"\n",
        "          )\n",
        "          self.use_llm_classifier = False\n",
        "      except:\n",
        "          # Opción 2: Clasificador basado en LLM con Few-Shot Prompting\n",
        "          self.use_llm_classifier = True\n",
        "\n",
        "      # Definir intenciones con ejemplos para few-shot\n",
        "      self.intents = {\n",
        "          \"document_search\": {\n",
        "              \"description\": \"Consultas sobre reglas, manuales y cómo jugar\",\n",
        "              \"examples\": [\n",
        "                  \"¿Dónde puedo encontrar las reglas del juego?\",\n",
        "                  \"¿Cómo se configura el tablero al inicio?\",\n",
        "                  \"Explica la mecánica de construcción de edificios\"\n",
        "              ]\n",
        "          },\n",
        "          \"statistics\": {\n",
        "              \"description\": \"Consultas sobre estadísticas y datos numéricos\",\n",
        "              \"examples\": [\n",
        "                  \"¿Cuál es el rating promedio del juego?\",\n",
        "                  \"Muestra los juegos con mayor puntuación\",\n",
        "                  \"¿Cuántas reseñas positivas hay?\"\n",
        "              ]\n",
        "          },\n",
        "          \"relations\": {\n",
        "              \"description\": \"Consultas sobre relaciones entre elementos\",\n",
        "              \"examples\": [\n",
        "                  \"¿Qué diseñadores trabajaron en expansiones?\",\n",
        "                  \"Muestra las conexiones entre mecánicas de juego\",\n",
        "                  \"¿Qué expansiones están relacionadas con el diseño base?\"\n",
        "              ]\n",
        "          },\n",
        "          \"general\": {\n",
        "              \"description\": \"Consultas generales sobre el juego\",\n",
        "              \"examples\": [\n",
        "                  \"¿Qué es Tiny Towns?\",\n",
        "                  \"Dame información general sobre el juego\",\n",
        "                  \"¿Qué tipo de juego es Tiny Towns?\"\n",
        "              ]\n",
        "          }\n",
        "      }\n",
        "\n",
        "      print(\"Clasificador avanzado configurado\")\n",
        "\n",
        "  def classify_intent(self, query: str) -> str:\n",
        "      \"\"\"Clasificar intención usando el mejor clasificador disponible\"\"\"\n",
        "      if not self.use_llm_classifier:\n",
        "          # Usar modelo entrenado\n",
        "          result = self.intent_model(query)[0]\n",
        "          return result['label']\n",
        "      else:\n",
        "          # Usar LLM con few-shot prompting\n",
        "          prompt = \"Clasifica la siguiente consulta en una de estas categorías:\\n\"\n",
        "          for intent, data in self.intents.items():\n",
        "              prompt += f\"- {intent}: {data['description']}\\n\"\n",
        "              prompt += \"  Ejemplos:\\n\"\n",
        "              for example in data['examples']:\n",
        "                  prompt += f\"  * {example}\\n\"\n",
        "\n",
        "          prompt += f\"\\nConsulta a clasificar: \\\"{query}\\\"\\n\"\n",
        "          prompt += \"Devuelve solo el nombre de la categoría sin explicaciones.\"\n",
        "\n",
        "          # Llamar al LLM (implementación similar a generate_sql_filter)\n",
        "          try:\n",
        "              response = self._call_llm(prompt, max_tokens=10, temperature=0.1)\n",
        "              return response.strip().lower()\n",
        "          except:\n",
        "              # Fallback a clasificación por keywords\n",
        "              query_lower = query.lower()\n",
        "              for intent, data in self.intents.items():\n",
        "                  if any(keyword in query_lower for keyword in data['keywords']):\n",
        "                      return intent\n",
        "              return \"general\"\n",
        "\n",
        "  def _call_llm(self, prompt: str, max_tokens: int = 64, temperature: float = 0.7) -> str:\n",
        "        \"\"\"Función genérica para llamar al LLM\"\"\"\n",
        "        api_key = \"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\"\n",
        "        api_url = \"https://api-inference.huggingface.co/models/google/gemma-2b\"\n",
        "        headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "        chat_prompt = [\n",
        "            {\"role\": \"system\", \"content\": \"Eres un asistente útil que sigue instrucciones cuidadosamente.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        prompt_with_template = \"\".join(\n",
        "            f\"<|{msg['role']}|>{msg['content']}</s>\\n\" for msg in chat_prompt\n",
        "        ) + \"<|assistant|>\\n\"\n",
        "\n",
        "        data = {\n",
        "            \"inputs\": prompt_with_template,\n",
        "            \"parameters\": {\n",
        "                \"max_new_tokens\": max_tokens,\n",
        "                \"temperature\": temperature,\n",
        "                \"top_k\": 50,\n",
        "                \"top_p\": 0.95,\n",
        "                \"do_sample\": True\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(api_url, headers=headers, json=data, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response.json()[0]['generated_text'].strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error llamando al LLM: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "  def process_query(self, query: str, k: int = 5) -> Dict:\n",
        "      \"\"\"Procesar consulta completa del RAG\"\"\"\n",
        "      # Clasificar intención\n",
        "      intent = self.classify_intent(query)\n",
        "\n",
        "      result = {\n",
        "          \"query\": query,\n",
        "          \"intent\": intent,\n",
        "          \"response\": \"\",\n",
        "          \"sources\": []\n",
        "      }\n",
        "\n",
        "      if intent == \"document_search\":\n",
        "          # Búsqueda híbrida en documentos\n",
        "          search_results = self.hybrid_search(query, k)\n",
        "          reranked_results = self.rerank_results(query, search_results)\n",
        "\n",
        "          result[\"sources\"] = reranked_results\n",
        "          result[\"response\"] = self._generate_response(query, reranked_results)\n",
        "\n",
        "      elif intent == \"statistics\":\n",
        "          # Consulta estadística\n",
        "          stats_result = self.query_statistics(query)\n",
        "          result[\"sources\"] = [stats_result]\n",
        "          result[\"response\"] = self._generate_stats_response(query, stats_result)\n",
        "\n",
        "      elif intent == \"relations\":\n",
        "          # Consulta de grafos\n",
        "          graph_result = self.query_graph(query)\n",
        "          result[\"sources\"] = [graph_result]\n",
        "          result[\"response\"] = self._generate_graph_response(query, graph_result)\n",
        "\n",
        "      else:\n",
        "          # Búsqueda general\n",
        "          search_results = self.hybrid_search(query, k)\n",
        "          result[\"sources\"] = search_results\n",
        "          result[\"response\"] = self._generate_response(query, search_results)\n",
        "\n",
        "      return result\n",
        "\n",
        "  def _generate_response(self, query: str, sources: List[Dict]) -> str:\n",
        "      \"\"\"Generar respuesta mejorada usando LLM\"\"\"\n",
        "      if not sources:\n",
        "          return \"No encontré información relevante. ¿Podrías reformular tu pregunta?\"\n",
        "\n",
        "      # Preparar contexto\n",
        "      context = \"Fuentes de información:\\n\"\n",
        "      for i, source in enumerate(sources[:3], 1):\n",
        "          context += f\"{i}. {source['text'][:300]}... (Fuente: {source['metadata']['source']})\\n\"\n",
        "\n",
        "      # Generar prompt para el LLM\n",
        "      prompt = (\n",
        "          \"Eres un experto en el juego de mesa Tiny Towns. \"\n",
        "          \"A continuación tienes información relevante y una pregunta del usuario.\\n\\n\"\n",
        "          f\"Información relevante:\\n{context}\\n\\n\"\n",
        "          f\"Pregunta del usuario: {query}\\n\\n\"\n",
        "          \"Proporciona una respuesta completa y útil basada en la información dada. \"\n",
        "          \"Si la información no es suficiente, sugiere reformular la pregunta. \"\n",
        "          \"Responde en el mismo idioma de la consulta.\"\n",
        "      )\n",
        "\n",
        "      # Llamar al LLM\n",
        "      response = self._call_llm(prompt, max_tokens=256)\n",
        "      return response if response else \"No pude generar una respuesta. Por favor intenta con otra pregunta.\"\n",
        "\n",
        "  def _generate_stats_response(self, query: str, stats_result: Dict) -> str:\n",
        "      \"\"\"Generar respuesta mejorada para estadísticas\"\"\"\n",
        "      if not stats_result[\"success\"]:\n",
        "          return f\"No pude procesar tu consulta estadística: {stats_result['error']}\"\n",
        "\n",
        "      data = stats_result[\"data\"]\n",
        "      if not data:\n",
        "          return \"No encontré datos que coincidan con tu consulta.\"\n",
        "\n",
        "      # Preparar resumen de datos\n",
        "      summary = f\"Encontré {stats_result['count']} registros relevantes:\\n\"\n",
        "      for i, record in enumerate(data[:5], 1):\n",
        "          summary += f\"{i}. {str(record)}\\n\"\n",
        "\n",
        "      # Generar respuesta con LLM\n",
        "      prompt = (\n",
        "          \"Eres un analista de datos de juegos de mesa. \"\n",
        "          \"A continuación tienes una pregunta y datos relevantes.\\n\\n\"\n",
        "          f\"Pregunta: {query}\\n\\n\"\n",
        "          f\"Datos encontrados:\\n{summary}\\n\\n\"\n",
        "          \"Genera un resumen conciso que responda la pregunta usando los datos. \"\n",
        "          \"Destaca los valores más importantes. \"\n",
        "          \"Responde en el mismo idioma de la consulta.\"\n",
        "      )\n",
        "\n",
        "      response = self._call_llm(prompt, max_tokens=200)\n",
        "      return response if response else summary\n",
        "\n",
        "  def _generate_graph_response(self, query: str, graph_result: Dict) -> str:\n",
        "      \"\"\"Generar respuesta mejorada para grafos\"\"\"\n",
        "      if not graph_result[\"success\"]:\n",
        "          return f\"Error al consultar relaciones: {graph_result['error']}\"\n",
        "\n",
        "      nodes = graph_result.get(\"nodes\", [])\n",
        "      edges = graph_result.get(\"edges\", [])\n",
        "\n",
        "      if not nodes and not edges:\n",
        "          return \"No encontré relaciones relevantes para tu consulta.\"\n",
        "\n",
        "      # Preparar resumen de relaciones\n",
        "      summary = \"Relaciones encontradas:\\n\"\n",
        "      if nodes:\n",
        "          summary += f\"- Nodos relevantes: {', '.join(str(n) for n in nodes[:5])}\\n\"\n",
        "      if edges:\n",
        "          summary += f\"- Conexiones: {', '.join(f'{e[0]} → {e[1]}' for e in edges[:5])}\\n\"\n",
        "\n",
        "      # Generar respuesta con LLM\n",
        "      prompt = (\n",
        "          \"Eres un experto en análisis de relaciones entre elementos de juegos. \"\n",
        "          \"A continuación tienes una pregunta y relaciones encontradas.\\n\\n\"\n",
        "          f\"Pregunta: {query}\\n\\n\"\n",
        "          f\"Relaciones:\\n{summary}\\n\\n\"\n",
        "          \"Explica las relaciones encontradas de manera clara y cómo responden a la pregunta. \"\n",
        "          \"Responde en el mismo idioma de la consulta.\"\n",
        "      )\n",
        "\n",
        "      response = self._call_llm(prompt, max_tokens=200)\n",
        "      return response if response else summary"
      ],
      "metadata": {
        "id": "suCTrNpI0W1m"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 2: Agente Autonomo"
      ],
      "metadata": {
        "id": "dNWHq6_e0w6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyTownsAgent:\n",
        "    \"\"\"Agente autónomo basado en ReAct para Tiny Towns\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system: TinyTownsRAG):\n",
        "        self.rag_system = rag_system\n",
        "        self.tools = []\n",
        "        self.agent = None\n",
        "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "        self._create_tools()\n",
        "        self._setup_agent()\n",
        "\n",
        "    def _create_tools(self):\n",
        "        \"\"\"Crear herramientas para el agente\"\"\"\n",
        "\n",
        "        # Herramienta de búsqueda en documentos\n",
        "        def doc_search(query: str) -> str:\n",
        "            \"\"\"Busca información en los documentos de Tiny Towns con búsqueda híbrida y re-ranking\"\"\"\n",
        "            results = self.rag_system.hybrid_search(query, k=5)\n",
        "            reranked = self.rag_system.rerank_results(query, results)\n",
        "\n",
        "            if not reranked:\n",
        "                return \"No se encontró información relevante en los documentos.\"\n",
        "\n",
        "            response = \"Información encontrada en documentos:\\n\"\n",
        "            for i, result in enumerate(reranked[:3]):\n",
        "                response += f\"{i+1}. {result['text'][:200]}...\\n\"\n",
        "\n",
        "            return response\n",
        "\n",
        "        # Herramienta de búsqueda en tablas\n",
        "        def table_search(query: str) -> str:\n",
        "            \"\"\"Realiza consultas dinámicas a los datos tabulares de Tiny Towns\"\"\"\n",
        "            stats_result = self.rag_system.query_statistics(query)\n",
        "\n",
        "            if stats_result[\"success\"]:\n",
        "                response = f\"Datos encontrados ({stats_result['count']} registros):\\n\"\n",
        "                for record in stats_result[\"data\"][:3]:\n",
        "                    response += f\"- {record}\\n\"\n",
        "                return response\n",
        "            else:\n",
        "                return f\"Error en consulta tabular: {stats_result['error']}\"\n",
        "\n",
        "        # Herramienta de búsqueda en grafos\n",
        "        def graph_search(query: str) -> str:\n",
        "            \"\"\"Realiza consultas dinámicas a la base de datos de grafos\"\"\"\n",
        "            graph_result = self.rag_system.query_graph(query)\n",
        "\n",
        "            if graph_result[\"success\"]:\n",
        "                response = f\"Relaciones encontradas:\\n\"\n",
        "                response += f\"Nodos: {graph_result['nodes'][:5]}\\n\"\n",
        "                response += f\"Conexiones: {graph_result['edges'][:5]}\\n\"\n",
        "                return response\n",
        "            else:\n",
        "                return f\"Error en consulta de grafos: {graph_result['error']}\"\n",
        "\n",
        "        # Crear objetos Tool\n",
        "        self.tools = [\n",
        "            Tool(\n",
        "                name=\"doc_search\",\n",
        "                description=\"Busca información en manuales, reglas y documentos sobre Tiny Towns\",\n",
        "                func=doc_search\n",
        "            ),\n",
        "            Tool(\n",
        "                name=\"table_search\",\n",
        "                description=\"Consulta estadísticas, ratings y datos tabulares sobre Tiny Towns\",\n",
        "                func=table_search\n",
        "            ),\n",
        "            Tool(\n",
        "                name=\"graph_search\",\n",
        "                description=\"Explora relaciones y conexiones entre elementos del juego\",\n",
        "                func=graph_search\n",
        "            ),\n",
        "            DuckDuckGoSearchRun(name=\"web_search\"),\n",
        "            WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "        ]\n",
        "\n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Configurar agente ReAct completo\"\"\"\n",
        "        from langchain.llms import HuggingFaceHub\n",
        "        from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "        # Configurar LLM\n",
        "        llm = HuggingFaceHub(\n",
        "            repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "            huggingfacehub_api_token=\"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\",\n",
        "            model_kwargs={\n",
        "                \"temperature\": 0.3,\n",
        "                \"max_new_tokens\": 256,\n",
        "                \"top_p\": 0.95\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Crear prompt template\n",
        "        template = \"\"\"Eres un asistente experto en el juego de mesa Tiny Towns. Responde siempre en el mismo idioma de la consulta.\n",
        "\n",
        "        Herramientas disponibles:\n",
        "        {tools}\n",
        "        Nombres de herramientas: {tool_names}\n",
        "\n",
        "        Instrucciones:\n",
        "        - Analiza cuidadosamente la pregunta\n",
        "        - Usa las herramientas necesarias para obtener información\n",
        "        - Si una herramienta no da resultados, prueba con otra\n",
        "        - Combina información de múltiples herramientas si es necesario\n",
        "        - Si no encuentras información relevante, sugiere reformular la pregunta\n",
        "\n",
        "        Historial de conversación:\n",
        "        {chat_history}\n",
        "\n",
        "        Pregunta: {input}\n",
        "        Thought: {agent_scratchpad}\"\"\"\n",
        "\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        # Configurar agente\n",
        "        self.agent = create_react_agent(llm, self.tools, prompt)\n",
        "        self.agent_executor = AgentExecutor(\n",
        "            agent=self.agent,\n",
        "            tools=self.tools,\n",
        "            memory=self.memory,\n",
        "            verbose=True,\n",
        "            handle_parsing_errors=True\n",
        "        )\n",
        "\n",
        "        print(\"Agente ReAct configurado completamente\")\n",
        "\n",
        "    def chat(self, query: str) -> str:\n",
        "        \"\"\"Procesar consulta con el agente completo\"\"\"\n",
        "        try:\n",
        "            response = self.agent_executor.invoke({\"input\": query})\n",
        "            return response[\"output\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error en el agente: {e}\")\n",
        "            # Fallback al RAG directo\n",
        "            result = self.rag_system.process_query(query)\n",
        "            return result[\"response\"]\n"
      ],
      "metadata": {
        "id": "nDA6x7sD087T"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funcion de evaluacion"
      ],
      "metadata": {
        "id": "N_40vvo30-KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_evaluation_tests():\n",
        "    \"\"\"Ejecutar pruebas de evaluación completas\"\"\"\n",
        "    print(\"Iniciando evaluación completa del sistema...\")\n",
        "\n",
        "    # Inicializar sistemas\n",
        "    rag_system = TinyTownsRAG()\n",
        "    agent = TinyTownsAgent(rag_system)\n",
        "\n",
        "    # Casos de prueba para evaluar todos los componentes\n",
        "    test_cases = [\n",
        "        # Consultas de documentos\n",
        "        {\n",
        "            \"query\": \"¿Cómo se configura el tablero al inicio de una partida de Tiny Towns?\",\n",
        "            \"type\": \"document\",\n",
        "            \"expected_intent\": \"document_search\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Explica las reglas para construir edificios en el juego\",\n",
        "            \"type\": \"document\",\n",
        "            \"expected_intent\": \"document_search\"\n",
        "        },\n",
        "        # Consultas estadísticas\n",
        "        {\n",
        "            \"query\": \"¿Cuál es la puntuación promedio de Tiny Towns según las reseñas?\",\n",
        "            \"type\": \"statistics\",\n",
        "            \"expected_intent\": \"statistics\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Muestra los 5 juegos con mayor rating de dificultad\",\n",
        "            \"type\": \"statistics\",\n",
        "            \"expected_intent\": \"statistics\"\n",
        "        },\n",
        "        # Consultas de relaciones\n",
        "        {\n",
        "            \"query\": \"¿Qué diseñadores trabajaron en expansiones del juego base?\",\n",
        "            \"type\": \"relations\",\n",
        "            \"expected_intent\": \"relations\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Muestra las conexiones entre las mecánicas de juego y los diseñadores\",\n",
        "            \"type\": \"relations\",\n",
        "            \"expected_intent\": \"relations\"\n",
        "        },\n",
        "        # Consultas generales\n",
        "        {\n",
        "            \"query\": \"¿Qué tipo de juego es Tiny Towns?\",\n",
        "            \"type\": \"general\",\n",
        "            \"expected_intent\": \"general\"\n",
        "        },\n",
        "        # Consultas complejas para el agente\n",
        "        {\n",
        "            \"query\": \"Compara Tiny Towns con otros juegos de construcción de ciudades en términos de complejidad y rating\",\n",
        "            \"type\": \"complex\",\n",
        "            \"expected_intent\": None  # El agente debe decidir\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Busca información sobre la última expansión de Tiny Towns y dime cuándo fue lanzada\",\n",
        "            \"type\": \"complex\",\n",
        "            \"expected_intent\": None\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Evaluar RAG\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EVALUACIÓN SISTEMA RAG\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    rag_results = []\n",
        "    for i, test in enumerate(test_cases[:6], 1):  # Probar solo las primeras 6 (simples)\n",
        "        print(f\"\\nPrueba {i}: {test['query']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        result = rag_system.process_query(test['query'])\n",
        "\n",
        "        # Verificar intención detectada\n",
        "        intent_match = result['intent'] == test['expected_intent']\n",
        "        print(f\"Intención: {result['intent']} (Esperada: {test['expected_intent']}) → {'✅' if intent_match else '❌'}\")\n",
        "\n",
        "        # Verificar respuesta\n",
        "        print(f\"Respuesta: {result['response'][:100]}...\")\n",
        "\n",
        "        # Verificar fuentes\n",
        "        sources_ok = len(result['sources']) > 0\n",
        "        print(f\"Fuentes: {len(result['sources'])} → {'✅' if sources_ok else '❌'}\")\n",
        "\n",
        "        # Guardar resultados\n",
        "        rag_results.append({\n",
        "            \"test\": i,\n",
        "            \"query\": test['query'],\n",
        "            \"intent_match\": intent_match,\n",
        "            \"response_length\": len(result['response']),\n",
        "            \"sources_found\": sources_ok\n",
        "        })\n",
        "\n",
        "    # Evaluar Agente\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EVALUACIÓN AGENTE AUTÓNOMO\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    agent_results = []\n",
        "    for i, test in enumerate(test_cases, 1):\n",
        "        print(f\"\\nPrueba {i}: {test['query']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        response = agent.chat(test['query'])\n",
        "\n",
        "        # Verificar respuesta\n",
        "        response_ok = len(response) > 20  # Respuesta no vacía\n",
        "        print(f\"Respuesta: {response[:100]}... → {'✅' if response_ok else '❌'}\")\n",
        "\n",
        "        # Guardar resultados\n",
        "        agent_results.append({\n",
        "            \"test\": i,\n",
        "            \"query\": test['query'],\n",
        "            \"response_ok\": response_ok,\n",
        "            \"response_length\": len(response)\n",
        "        })\n",
        "\n",
        "    # Mostrar resumen\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"RESUMEN DE EVALUACIÓN\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Estadísticas RAG\n",
        "    rag_intent_accuracy = sum(1 for r in rag_results if r['intent_match']) / len(rag_results)\n",
        "    rag_sources_found = sum(1 for r in rag_results if r['sources_found']) / len(rag_results)\n",
        "\n",
        "    print(f\"\\nSistema RAG:\")\n",
        "    print(f\"- Precisión de intención: {rag_intent_accuracy:.1%}\")\n",
        "    print(f\"- Fuentes encontradas: {rag_sources_found:.1%}\")\n",
        "    print(f\"- Longitud promedio de respuesta: {sum(r['response_length'] for r in rag_results)/len(rag_results):.0f} chars\")\n",
        "\n",
        "    # Estadísticas Agente\n",
        "    agent_success = sum(1 for r in agent_results if r['response_ok']) / len(agent_results)\n",
        "\n",
        "    print(f\"\\nAgente Autónomo:\")\n",
        "    print(f\"- Consultas exitosas: {agent_success:.1%}\")\n",
        "    print(f\"- Longitud promedio de respuesta: {sum(r['response_length'] for r in agent_results)/len(agent_results):.0f} chars\")\n",
        "\n",
        "    print(\"\\nEvaluación completada\")"
      ],
      "metadata": {
        "id": "kjk0rHaL1I5D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecucion del sistema"
      ],
      "metadata": {
        "id": "5CsdzOJP1iwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"SISTEMA RAG Y AGENTE AUTÓNOMO - TINY TOWNS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Ejecutar evaluación\n",
        "    run_evaluation_tests()\n",
        "\n",
        "    print(\"\\nSistema listo para uso interactivo\")\n",
        "    print(\"Puedes usar las clases TinyTownsRAG y TinyTownsAgent para consultas\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zr3g9CGW7W8",
        "outputId": "3cec9747-da26-4e5b-a034-7b54d445de0c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SISTEMA RAG Y AGENTE AUTÓNOMO - TINY TOWNS\n",
            "==================================================\n",
            "Iniciando evaluación completa del sistema...\n",
            "Cargando datos...\n",
            "Datos cargados exitosamente\n",
            "Configurando modelo de embeddings...\n",
            "Modelo de embeddings configurado\n",
            "Creando base de datos vectorial...\n",
            "Base de datos vectorial creada con 123 chunks\n",
            "Configurando interfaz estadística...\n",
            "Interfaz estadística configurada\n",
            "Configurando base de datos de grafos...\n",
            "Error configurando Neo4j: Cannot open connection to ConnectionProfile('bolt://localhost:7687')\n",
            "Configurando clasificador de intención avanzado...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clasificador avanzado configurado\n",
            "Agente ReAct configurado completamente\n",
            "\n",
            "==================================================\n",
            "EVALUACIÓN SISTEMA RAG\n",
            "==================================================\n",
            "\n",
            "Prueba 1: ¿Cómo se configura el tablero al inicio de una partida de Tiny Towns?\n",
            "--------------------------------------------------\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Intención:  (Esperada: document_search) → ❌\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta....\n",
            "Fuentes: 5 → ✅\n",
            "\n",
            "Prueba 2: Explica las reglas para construir edificios en el juego\n",
            "--------------------------------------------------\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Intención:  (Esperada: document_search) → ❌\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta....\n",
            "Fuentes: 5 → ✅\n",
            "\n",
            "Prueba 3: ¿Cuál es la puntuación promedio de Tiny Towns según las reseñas?\n",
            "--------------------------------------------------\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Intención:  (Esperada: statistics) → ❌\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta....\n",
            "Fuentes: 5 → ✅\n",
            "\n",
            "Prueba 4: Muestra los 5 juegos con mayor rating de dificultad\n",
            "--------------------------------------------------\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Intención:  (Esperada: statistics) → ❌\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta....\n",
            "Fuentes: 5 → ✅\n",
            "\n",
            "Prueba 5: ¿Qué diseñadores trabajaron en expansiones del juego base?\n",
            "--------------------------------------------------\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Intención:  (Esperada: relations) → ❌\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta....\n",
            "Fuentes: 5 → ✅\n",
            "\n",
            "Prueba 6: Muestra las conexiones entre las mecánicas de juego y los diseñadores\n",
            "--------------------------------------------------\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Intención:  (Esperada: relations) → ❌\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta....\n",
            "Fuentes: 5 → ✅\n",
            "\n",
            "==================================================\n",
            "EVALUACIÓN AGENTE AUTÓNOMO\n",
            "==================================================\n",
            "\n",
            "Prueba 1: ¿Cómo se configura el tablero al inicio de una partida de Tiny Towns?\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Error en el agente: 'InferenceClient' object has no attribute 'post'\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta.... → ✅\n",
            "\n",
            "Prueba 2: Explica las reglas para construir edificios en el juego\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Error en el agente: 'InferenceClient' object has no attribute 'post'\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta.... → ✅\n",
            "\n",
            "Prueba 3: ¿Cuál es la puntuación promedio de Tiny Towns según las reseñas?\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Error en el agente: 'InferenceClient' object has no attribute 'post'\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta.... → ✅\n",
            "\n",
            "Prueba 4: Muestra los 5 juegos con mayor rating de dificultad\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Error en el agente: 'InferenceClient' object has no attribute 'post'\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta.... → ✅\n",
            "\n",
            "Prueba 5: ¿Qué diseñadores trabajaron en expansiones del juego base?\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Error en el agente: 'InferenceClient' object has no attribute 'post'\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta.... → ✅\n",
            "\n",
            "Prueba 6: Muestra las conexiones entre las mecánicas de juego y los diseñadores\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Error en el agente: 'InferenceClient' object has no attribute 'post'\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta.... → ✅\n",
            "\n",
            "Prueba 7: ¿Qué tipo de juego es Tiny Towns?\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Error en el agente: 'InferenceClient' object has no attribute 'post'\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta.... → ✅\n",
            "\n",
            "Prueba 8: Compara Tiny Towns con otros juegos de construcción de ciudades en términos de complejidad y rating\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Error en el agente: 'InferenceClient' object has no attribute 'post'\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta.... → ✅\n",
            "\n",
            "Prueba 9: Busca información sobre la última expansión de Tiny Towns y dime cuándo fue lanzada\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Error en el agente: 'InferenceClient' object has no attribute 'post'\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Error llamando al LLM: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/google/gemma-2b\n",
            "Respuesta: No pude generar una respuesta. Por favor intenta con otra pregunta.... → ✅\n",
            "\n",
            "==================================================\n",
            "RESUMEN DE EVALUACIÓN\n",
            "==================================================\n",
            "\n",
            "Sistema RAG:\n",
            "- Precisión de intención: 0.0%\n",
            "- Fuentes encontradas: 100.0%\n",
            "- Longitud promedio de respuesta: 67 chars\n",
            "\n",
            "Agente Autónomo:\n",
            "- Consultas exitosas: 100.0%\n",
            "- Longitud promedio de respuesta: 67 chars\n",
            "\n",
            "Evaluación completada\n",
            "\n",
            "Sistema listo para uso interactivo\n",
            "Puedes usar las clases TinyTownsRAG y TinyTownsAgent para consultas\n"
          ]
        }
      ]
    }
  ]
}